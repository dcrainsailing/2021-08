{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "continuous-infection",
   "metadata": {},
   "source": [
    "一.boosting\n",
    "\n",
    "    思想\n",
    "\n",
    "其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。 2.代表模型及其原理 AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。 而GBDT-梯度提升决策树树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。\n",
    "\n",
    "二.bagging(bootstrap aggregating) 1.思想\n",
    "\n",
    "Bagging即套袋法，其思想是: A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "\n",
    "B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "\n",
    "C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n",
    "\n",
    "2.代表模型及其原理 随机森林(RF) 1.随机森林的弱学习器都是决策树。 2.随机森林在bagging的样本随机采样的基础上,又加上了特征的随机选择。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-newman",
   "metadata": {},
   "source": [
    "# 什么是偏差和方差分解？偏差是谁的偏差？此处的方差又是指什么？\n",
    "\n",
    "偏差：the difference between your model’s expected predictions and the true values. 衡量了模型期望输出与真实值之间的差别，刻画了模型本身的拟合能力。 \n",
    "\n",
    "方差：refers to your algorithm’s sensitivity to specific sets of training data. High variance algorithms will produce drastically different models depending on the training set. 度量了训练集的变动所导致的学习性能的变化，刻画了模型输出结果由于训练集的不同造成的波动。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-petersburg",
   "metadata": {},
   "source": [
    "# 相较于使用单个模型，bagging和boosting方法有何优势？\n",
    "\n",
    "根据个体学习器生成方式的不同，目前集成学习方法大致可分为两大类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成的序列化方法，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。\n",
    "\n",
    "集成学习之bagging\n",
    "\n",
    "Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成。\n",
    "\n",
    "\n",
    "![](_images/jc3.jpeg)\n",
    "\n",
    "从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过3次的随机采样，我们就可以得到3个采样集，对于这3个采样集，我们可以分别独立的训练出3个弱学习器，再对这3个弱学习器通过集合策略来得到最终的强学习器。\n",
    "\n",
    "对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstap sampling）,即对于 [公式] 个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集 [公式] 次，最终可以得到 [公式] 个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。\n",
    "\n",
    "注：Bootstrap方法是非常有用的一种统计学上的估计方法。 Bootstrap是一类非参Monte Carlo方法,其实质是对观测信息进行再抽样，进而对总体的分布特性进行统计推断。首先，Bootstrap通过重抽样，避免了Cross-Validation造成的样本减少问题，其次，Bootstrap也可以创造数据的随机性。Bootstrap是一种有放回的重复抽样方法，抽样策略就是简单的随机抽样。\n",
    "\n",
    "随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。其在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。\n",
    "\n",
    "集成学习之boosting \n",
    "\n",
    "![](_images/jc2.jpeg)\n",
    "\n",
    "Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。\n",
    "\n",
    "Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-miniature",
   "metadata": {},
   "source": [
    "# 请叙述stacking的集成流程，并指出blending方法和它的区别。\n",
    "\n",
    "stacking严格来说并不是一种算法，而是精美而又复杂的，对模型集成的一种策略。Stacking集成算法可以理解为一个两层的集成，第一层含有多个基础分类器，把预测的结果(元特征)提供给第二层， 而第二层的分类器通常是逻辑回归，他把一层分类器的结果当做特征做拟合输出预测结果。\n",
    "\n",
    " Stacking 方法：\n",
    "\n",
    "将训练好的所有基模型对整个训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测：\n",
    "\n",
    "![](_images/stacking2.png)\n",
    "\n",
    "Blending相较于Stacking来说要简单一些，其流程大致分为以下几步：\n",
    "\n",
    "    将数据划分为训练集和测试集(test_set)，其中训练集需要再次划分为训练集(train_set)和验证集(val_set)；\n",
    "    创建第一层的多个模型，这些模型可以使同质的也可以是异质的；\n",
    "    使用train_set训练步骤2中的多个模型，然后用训练好的模型预测val_set和test_set得到val_predict, test_predict1；\n",
    "    创建第二层的模型,使用val_predict作为训练集训练第二层的模型；\n",
    "    使用第二层训练好的模型对第二层测试集test_predict1进行预测，该结果为整个测试集的结果\n",
    "\n",
    "1.2 Blending 图解\n",
    "\n",
    "![](_images/stacking3.png)\n",
    "\n",
    "Blending与Stacking对比\n",
    "\n",
    "Blending的优点在于：\n",
    "\n",
    "1.比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）\n",
    "\n",
    "2.避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集\n",
    "\n",
    "3.在团队建模过程中，不需要给队友分享自己的随机种子\n",
    "\n",
    "而缺点在于：\n",
    "\n",
    "1.使用了很少的数据（是划分hold-out作为测试集，并非cv）\n",
    "\n",
    "2.blender可能会过拟合（其实大概率是第一点导致的）\n",
    "\n",
    "3.stacking使用多次的CV会比较稳健"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-helping",
   "metadata": {},
   "source": [
    "# 什么是随机森林的oob得分？\n",
    "\n",
    "首先简单说一下什么是袋外样本oob (Out of bag)：在随机森林中，m个训练样本会通过bootstrap (有放回的随机抽样) 的抽样方式进行T次抽样每次抽样产生样本数为m的采样集，进入到并行的T个决策树中。这样有放回的抽样方式会导致有部分训练集中的样本(约36.8%)未进入决策树的采样集中，而这部分未被采集的的样本就是袋外数据oob。\n",
    "\n",
    "而这个袋外数据就可以用来检测模型的泛化能力，和交叉验证类似。可以理解成从train datasets 中分出来的validation datasets。\n",
    "\n",
    "对于单棵用采样集训练完成的决策树Ti，用袋外数据运行后会产生一个oob_score (返回的是R square来判断)，对每一棵决策树都重复上述操作，最终会得到T个oob_score，把这T和oob_score平均，最终得到的就是整个随机森林的oob_score\n",
    "\n",
    "![](_images/oob.png)\n",
    "\n",
    "如何用oob判断特征的重要性\n",
    "\n",
    "用oob样本在训练好的决策树T1上运行，可以得到袋外数据误差 e1，然后保持其他列不变, permutate(随机重排/洗牌)第 i 列特征的特征值或者加噪音在 oob中第 i 列特征的特征值上，得到袋外数据误差 ei2。\n",
    "\n",
    "计算特征X的重要性=∑(ei2-e1)/T。改变特征值之后ei2变动越大，说明该特征产生的影响越大，该特征值越重要。\n",
    "\n",
    "通过利用oob对每个特征分别permutation或加噪音，迭代进行，评估改变每个特征后的∑(ei2-e1)/T，然后做一个排序，值越高的特征越重要。\n",
    "(OOB Error is the number of wrongly classifying the OOB Sample.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-poverty",
   "metadata": {},
   "source": [
    "# 随机森林是如何集成多个决策树模型的？\n",
    "\n",
    "随机森林（Random Forest，以下简称RF）是bagging的一个特化进阶版。　\n",
    "\n",
    "    所谓的特化是因为随机森林使用了CART决策树作为基学习器。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征字段的随机选择。这样进一步增强了模型的泛化能力。\n",
    "    \n",
    "    随机森林包含两个关键词，一个是“随机”，一个就是“森林”。随机森林的基学习器是决策树，一个决策树称为“树”，那多个决策树聚集在一起便是森林了，这体现了随机森林算法的集成思想。 随机森林的\"随机\"有两层含义，即样本抽样的随机性和特征抽样的随机性。\n",
    "\n",
    "随机森林的原理如下图所示：\n",
    "\n",
    "![](_images/randomforest.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-plaintiff",
   "metadata": {},
   "source": [
    "# 请叙述孤立森林的算法原理和流程。\n",
    "\n",
    "孤立森林（Isolation Forest）算法是西瓜书作者周志华老师的团队研究开发的算法，一般用于结构化数据的异常检测。\n",
    "异常的定义\n",
    "\n",
    "针对于不同类型的异常，要用不同的算法来进行检测，而孤立森林算法主要针对的是连续型结构化数据中的异常点。\n",
    "\n",
    "使用孤立森林的前提是，将异常点定义为那些 “容易被孤立的离群点” —— 可以理解为分布稀疏，且距离高密度群体较远的点。\n",
    "从统计学来看，在数据空间里，若一个区域内只有分布稀疏的点，表示数据点落在此区域的概率很低，因此可以认为这些区域的点是异常的。\n",
    "\n",
    "也就是说，孤立森林算法的理论基础有两点：\n",
    "\n",
    "    异常数据占总样本量的比例很小；\n",
    "    异常点的特征值与正常点的差异很大。\n",
    "![](_images/isolandforest.jpeg)\n",
    "\n",
    "使用场景\n",
    "\n",
    "孤立森林算法是基于 Ensemble 的异常检测方法，因此具有线性的时间复杂度。且精准度较高，在处理大数据时速度快，所以目前在工业界的应用范围比较广。常见的场景包括：网络安全中的攻击检测、金融交易欺诈检测、疾病侦测、噪声数据过滤（数据清洗）等。\n",
    "与其他异常检测算法的差异\n",
    "\n",
    "孤立森林中的 “孤立” (isolation) 指的是 “把异常点从所有样本中孤立出来”，论文中的原文是 “separating an instance from the rest of the instances”.\n",
    "\n",
    "大多数基于模型的异常检测算法会先 ”规定“ 正常点的范围或模式，如果某个点不符合这个模式，或者说不在正常范围内，那么模型会将其判定为异常点。\n",
    "\n",
    "孤立森林的创新点包括以下四个：\n",
    "\n",
    "    Partial models：在训练过程中，每棵孤立树都是随机选取部分样本；\n",
    "    No distance or density measures：不同于 KMeans、DBSCAN 等算法，孤立森林不需要计算有关距离、密度的指标，可大幅度提升速度，减小系统开销；\n",
    "    Linear time complexity：因为基于 ensemble，所以有线性时间复杂度。通常树的数量越多，算法越稳定；\n",
    "    Handle extremely large data size：由于每棵树都是独立生成的，因此可部署在大规模分布式系统上来加速运算。\n",
    "\n",
    "算法思想\n",
    "\n",
    "想象这样一个场景，我们用一个随机超平面对一个数据空间进行切割，切一次可以生成两个子空间（也可以想象用刀切蛋糕）。接下来，我们再继续随机选取超平面，来切割第一步得到的两个子空间，以此循环下去，直到每子空间里面只包含一个数据点为止。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-negative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "active-adaptation",
   "metadata": {},
   "source": [
    "# Adaboost概述\n",
    "\n",
    "Adaboost的全称是Adaptive Boosting，其含义为自适应提升算法。其中，自适应是指Adaboost会根据本轮样本的误差结果来分配下一轮模型训练时样本\n",
    "\n",
    "在模型中的相对权重，即对错误的或偏差大的样本适度“重视”，对正确的或偏差小的样本适度“放松”，这里的“重视”和“放松”具体体现在了Adaboost的损失\n",
    "\n",
    "函数设计以及样本权重的更新策略。本课我们将介绍Adaboost处理分类和回归任务的算法原理，包括SAMME算法、SAMME.R算法和Adaboost.R2算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-oriental",
   "metadata": {},
   "source": [
    "2. 分类损失¶\n",
    "\n",
    "对于K\n",
    "分类问题而言，当样本标签$y=[y1,...,yK]T$的类别c为第k\n",
    "\n",
    "类时，记\n",
    "$yk=⎧⎩⎨1,−1k−1,if c=kif c≠k$\n",
    "\n",
    "【练习】假设有一个3分类问题，标签类别为第2类，模型输出的类别标签为[-0.1,-0.3,0.4]，请计算对应的指数损失。\n",
    "\n",
    "设模型的输出结果为$f=[f1,...,fK]T$\n",
    "\n",
    "，则记损失函数为\n",
    "$L(y,f)=exp(−yTfK)$\n",
    "\n",
    "由于对任意的向量a1\n",
    "\n",
    "有\n",
    "$L(y,f+a1)=exp(−yTfK−ayT1K)=exp(−yTfK)=L(y,f)$\n",
    "\n",
    "因此为了保证f\n",
    "\n",
    "的可估性，我们需要作出约束假设，此处选择对称约束条件\n",
    "$f1+f2+...+fK=0$\n",
    "\n",
    "从概率角度而言，一个设计良好的分类问题损失函数应当保证模型在期望损失达到最小时的输出结果是使得后验概率$P(c|x)$\n",
    "达到最大的类别，这个条件被称为贝叶斯最优决策条件。在本问题下，满足对称约束条件的损失函数期望损失$EY|xL(Y,f)$\n",
    "\n",
    "达到最小时，由拉格朗日乘子法可解得模型输出为\n",
    "$k∗=argmaxkf∗k(x)=argmaxk(K−1)[logP(c=k|x)−1K∑i=1KlogP(c=i|x)]=argmaxkP(c=k|x)$\n",
    "\n",
    "因此，选择指数损失能够满足贝叶斯最优决策条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-prize",
   "metadata": {},
   "source": [
    "【练习】假设有一个3分类问题，标签类别为第2类，模型输出的类别标签为[-0.1,-0.3,0.4]，请计算对应的指数损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-automation",
   "metadata": {},
   "source": [
    "【练习】对公式进行化简，写出K=2时的SAMME算法流程，并与李航《统计学习方法》一书中所述的Adaboost二分类算法对比是否一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-closing",
   "metadata": {},
   "source": [
    "【练习】在sklearn源码中找出算法流程中每一行对应的处理代码。\n",
    "\n",
    "【练习】算法2第12行中给出了f\n",
    "输出的迭代方案，但在sklearn包的实现中使用了I{G∗(x)=S(y)}来代替b∗(m)(x)。请根据本文的实现，对sklearn包的源码进行修改并构造一个例子来比较它们的输出是否会不同。（提示：修改AdaboostClassifier类中的decision_function函数和staged_decision_function函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-category",
   "metadata": {},
   "source": [
    "$h∗(m)k′=(K−1)[logPw(S(y)=k′|x)−1K∑k=1KlogP(S(y)=k|x)]$\n",
    "\n",
    "【练习】验证h∗k′的求解结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-training",
   "metadata": {},
   "source": [
    "【练习】算法3的第14行给出了wi的更新策略，请说明其合理性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-kentucky",
   "metadata": {},
   "source": [
    "【练习】请结合加权中位数的定义解决以下问题：\n",
    "\n",
    "    当满足什么条件时，Adaboost.R2的输出结果恰为每个基预测器输出值的中位数？\n",
    "\n",
    "    Adaboost.R2模型对测试样本的预测输出值是否一定会属于M\n",
    "\n",
    "个分类器中的一个输出结果？若是请说明理由，若不一定请给出反例。\n",
    "\n",
    "设k∈{y1,...,yM}\n",
    "，记k两侧（即大于或小于k）的样本集合对应的权重集合为W+和W−，证明使这两个集合元素之和差值最小的k就是Adaboost.R2输出的y\n",
    "\n",
    "。\n",
    "\n",
    "相对于普通中位数，加权中位数的输出结果鲁棒性更强，请结合公式说明理由。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-pearl",
   "metadata": {},
   "source": [
    " 二分类问题下，Adaboost算法如何调节样本的权重？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-lounge",
   "metadata": {},
   "source": [
    "AdaBoost，是英文\"Adaptive Boosting\"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。\n",
    "\n",
    "具体说来，整个Adaboost 迭代算法就3步：\n",
    "\n",
    "    初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。\n",
    "    \n",
    "    训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。\n",
    "    \n",
    "    将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 \n",
    "\n",
    "另外一种思路是：每次更新每个样本的权重，然后在下一个弱分类器的构造中所有样本参与训练，但是每个权值不一样，这可以通过将样本权值增加到神经网络的目标函数（均方误差MSE）中实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-cheese",
   "metadata": {},
   "source": [
    "样本A在当轮分类错误，且样本B在当轮分类正确，请问在权重调整后，样本A的权重一定大于样本B吗？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "在处理分类问题时，Adaboost的损失函数是什么？请叙述其设计的合理性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaboost如何处理回归问题？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-communications",
   "metadata": {},
   "source": [
    " 用已经训练完毕的Adaboost分类模型和回归的模型来预测新样本的标签，请分别具体描述样本从输入到标签输出的流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-missouri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn中SAMME的验证集得分为:  0.8316666666666667\n",
      "使用SAMME.R集成的验证集得分为:  0.8316666666666667\n",
      "sklearn中SAMME.R的验证集得分为:  0.8503333333333334\n",
      "使用SAMME.R集成的验证集得分为:  0.8503333333333334\n",
      "使用决策树桩的验证集得分为:  0.7746666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaboostClassifier:\n",
    "\n",
    "    def __init__(self, base_estimator, n_estimators, algorithm):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "        self.boostors = []\n",
    "        if self.algorithm == \"SAMME\":\n",
    "            self.boostor_weights = []\n",
    "\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        w = np.repeat(1/X.shape[0], X.shape[0])\n",
    "        self.classes = np.unique(y.reshape(-1)).shape[0]\n",
    "        output = 0\n",
    "        for n in range(self.n_estimators):\n",
    "            cur_boostor = self.base_estimator(**kwargs)\n",
    "            cur_boostor.fit(X, y, w)\n",
    "            if self.algorithm == \"SAMME\":\n",
    "                y_pred = cur_boostor.predict(X)\n",
    "                err = (w*(y != y_pred)).sum()\n",
    "                alpha = np.log((1-err)/err) + np.log(self.classes-1)\n",
    "                temp_output = np.full(\n",
    "                    (X.shape[0], self.classes), -1/(self.classes-1))\n",
    "                temp_output[np.arange(X.shape[0]), y_pred] = 1\n",
    "                self.boostors.append(cur_boostor)\n",
    "                self.boostor_weights.append(alpha)\n",
    "                w *= np.exp(alpha * (y != y_pred))\n",
    "                w /= w.sum()\n",
    "                output += temp_output * alpha\n",
    "            elif self.algorithm == \"SAMME.R\":\n",
    "                y_pred = cur_boostor.predict_proba(X)\n",
    "                log_proba = np.log(y_pred + 1e-6)\n",
    "                temp_output = (\n",
    "                    self.classes-1)*(log_proba-log_proba.mean(1).reshape(-1,1))\n",
    "                temp_y = np.full(\n",
    "                    (X.shape[0], self.classes), -1/(self.classes-1))\n",
    "                temp_y[np.arange(X.shape[0]), y] = 1\n",
    "                self.boostors.append(cur_boostor)\n",
    "                w *= np.exp(\n",
    "                    (1-self.classes)/self.classes * (temp_y*log_proba).sum(1))\n",
    "                w /= w.sum()\n",
    "                output += temp_output\n",
    "            #acc = accuracy_score(y, np.argmax(output, axis=1))\n",
    "            #print(acc)\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = 0\n",
    "        if self.algorithm == \"SAMME\":\n",
    "            for n in range(self.n_estimators):\n",
    "                cur_pred = self.boostors[n].predict(X)\n",
    "                temp_output = np.full(\n",
    "                    (X.shape[0], self.classes), -1/(self.classes-1))\n",
    "                temp_output[np.arange(X.shape[0]), cur_pred] = 1\n",
    "                result += self.boostor_weights[n] * temp_output\n",
    "        elif self.algorithm == \"SAMME.R\":\n",
    "            for n in range(self.n_estimators):\n",
    "                y_pred = self.boostors[n].predict_proba(X)\n",
    "                log_proba = np.log(y_pred + 1e-6)\n",
    "                temp_output = (\n",
    "                    self.classes-1)*(log_proba-log_proba.mean(1).reshape(-1,1))\n",
    "                result += temp_output\n",
    "        return np.argmax(result, axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=10000, n_features=10,\n",
    "        n_informative=5, random_state=0, n_classes=2\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=0\n",
    "    )\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "    clf = ABC(\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=20, algorithm=\"SAMME\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"sklearn中SAMME的验证集得分为: \", accuracy_score(y_test, result))\n",
    "\n",
    "    clf = AdaboostClassifier(\n",
    "        DecisionTreeClassifier,\n",
    "        20, \"SAMME\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train, max_depth=1)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"使用SAMME.R集成的验证集得分为: \", accuracy_score(y_test, result))\n",
    "\n",
    "    clf = ABC(\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=20, algorithm=\"SAMME.R\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"sklearn中SAMME.R的验证集得分为: \", accuracy_score(y_test, result))\n",
    "\n",
    "    clf = AdaboostClassifier(\n",
    "        DecisionTreeClassifier,\n",
    "        20, \"SAMME.R\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train, max_depth=1)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"使用SAMME.R集成的验证集得分为: \", accuracy_score(y_test, result))\n",
    "\n",
    "    clf = DecisionTreeClassifier(max_depth=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"使用决策树桩的验证集得分为: \", accuracy_score(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-documentary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
